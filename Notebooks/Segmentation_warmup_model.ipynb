{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mL-8Kfotzgou"
   },
   "source": [
    "# Brain Tumor Multiclass Segmentation (2D MRI) — Warm-Up Phase\n",
    "\n",
    "This notebook presents the *warm-up phase* of a deep learning pipeline for multiclass brain tumor segmentation using 2D MRI scans. The task is formulated as a **semantic segmentation** problem, where each pixel is classified into one of several tumor-related classes: background, edema, enhancing tumor, or necrotic core.\n",
    "\n",
    "We implement a U-Net architecture in **PyTorch**, with an **EfficientNet-B1** encoder pretrained on ImageNet. During this phase, the encoder is kept **frozen**, allowing the decoder and segmentation head to adapt first — a strategy that helps stabilize early training and prevent overfitting when using pretrained weights.\n",
    "\n",
    "The dataset is a 2D preprocessed variant of the **BraTS** dataset, re-hosted on [Kaggle](https://www.kaggle.com/code/balakrishcodes/brain-mri-2d/input). It comprises axial MRI slices along with their corresponding multiclass segmentation masks.\n",
    "\n",
    "This notebook is part of a personal research project focused on exploring **medical image segmentation** through the lens of **transfer learning**, **modular model design**, and **systematic experimentation**.\n",
    "\n",
    "> **Disclaimer:** This project is intended for research and educational purposes only. It is not designed for clinical or diagnostic use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DjcMJH7P8BD"
   },
   "source": [
    "## 0. Environment Setup (Google Colab)\n",
    "\n",
    "This notebook is intended to be executed on **Google Colab**. To access the dataset and save outputs such as trained models or logs, we mount **Google Drive**.\n",
    "\n",
    "> **Note:** If you are running this notebook outside of Colab (e.g., on a local machine or cloud server), ensure that the dataset is available in the expected directory structure and **skip this step**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2Zmi8nMQAwF"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive to access datasets and save model outputs\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qD2urdf5JHbm"
   },
   "source": [
    "## 1. Install and Import Dependencies\n",
    "\n",
    "This section sets up the environment by installing and importing all necessary libraries for building a **2D brain tumor segmentation model** using PyTorch and supporting frameworks.\n",
    "\n",
    "The following libraries are used throughout this project:\n",
    "\n",
    "- **Albumentations**: A fast and flexible image augmentation library that supports mask-aware transformations, commonly used in computer vision pipelines.\n",
    "- **Segmentation Models PyTorch (SMP)**: Provides a collection of high-level segmentation architectures (e.g., U-Net, DeepLabV3, FPN), often with pretrained encoders on ImageNet.\n",
    "- **TorchMetrics**: Offers standardized and modular evaluation metrics (e.g., Intersection over Union, Dice Score) that integrate seamlessly with PyTorch.\n",
    "- **MONAI**: A domain-specific framework for medical imaging, offering specialized components such as loss functions (e.g., Dice Loss), transforms, and dataset utilities.\n",
    "- **Scikit-learn**: Useful for classical ML utilities such as train-validation splitting, class balancing, and metric computation.\n",
    "- **Other utilities**: Libraries such as NumPy, OpenCV, tqdm, `h5py`, and `pickle` are used for I/O operations, preprocessing, and visualization.\n",
    "\n",
    "> This modular setup ensures that the pipeline remains **scalable**, **reproducible**, and **extensible** across different stages of experimentation and deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eubRQosHJIro"
   },
   "source": [
    "### 1.1 Install Required Packages\n",
    "\n",
    "This step installs the essential Python packages needed to support the brain tumor segmentation pipeline.\n",
    "\n",
    "- **Albumentations**: A fast and flexible library for image augmentation, including mask-aware transformations suitable for segmentation tasks.\n",
    "- **Segmentation Models PyTorch (SMP)**: Provides high-level segmentation architectures such as U-Net, DeepLabV3, and FPN, with pretrained encoder support (e.g., ImageNet).\n",
    "- **TorchMetrics**: Offers reliable and standardized evaluation metrics (e.g., IoU, Dice) compatible with PyTorch workflows.\n",
    "- **MONAI**: A domain-specific framework for medical imaging, offering specialized components like loss functions, transforms, and dataset utilities.\n",
    "\n",
    "> **Note:** If you're running this notebook outside of Google Colab, make sure to install these packages manually in your local environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "QuVILUOezwoG"
   },
   "outputs": [],
   "source": [
    "!pip install albumentations segmentation-models-pytorch torchmetrics monai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tiqb8jfNz_A8"
   },
   "source": [
    "### 1.2 Import Libraries\n",
    "\n",
    "This section imports all required Python libraries for model development, training, evaluation, and data processing throughout the notebook.\n",
    "\n",
    "- **PyTorch**: Core deep learning framework used to define the model architecture, loss functions, optimizers, and training logic.\n",
    "- **Albumentations**: High-performance library for data augmentation — supports image-mask pair transformations crucial for segmentation.\n",
    "- **TorchMetrics**: Provides standardized evaluation metrics such as IoU (Jaccard Index) for multiclass segmentation tasks.\n",
    "- **MONAI**: Specialized medical imaging toolkit offering domain-specific loss functions, transforms, and utilities.\n",
    "- **Segmentation Models PyTorch (SMP)**: Pretrained segmentation architectures (e.g., U-Net, FPN) with ImageNet-initialized encoders.\n",
    "- **scikit-learn**: Tools for data splitting, computing class weights, and basic evaluation utilities.\n",
    "- **Automatic Mixed Precision (AMP)**: PyTorch utilities (`autocast`, `GradScaler`) to accelerate training and reduce memory usage.\n",
    "- **General-purpose utilities**: Libraries such as NumPy, OpenCV, tqdm, h5py, and pickle for data manipulation, visualization, and file I/O.\n",
    "\n",
    "> **Note:** This environment is designed for use in **Google Colab**. Functions like Google Drive mounting and file uploads assume a Colab context. If running elsewhere, please adjust the setup accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBBu80jq0BLU"
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Core PyTorch Framework\n",
    "# -------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation Metrics & Class Handling\n",
    "# -------------------------\n",
    "from torchmetrics.classification import MulticlassJaccardIndex  # IoU metric for multiclass segmentation\n",
    "from sklearn.utils.class_weight import compute_class_weight      # Compute class weights for imbalance\n",
    "\n",
    "# -------------------------\n",
    "# Data Augmentation\n",
    "# -------------------------\n",
    "import albumentations as A                         # General image augmentation\n",
    "from albumentations.pytorch import ToTensorV2      # Convert images to PyTorch tensors\n",
    "\n",
    "# -------------------------\n",
    "# Medical Imaging Tools\n",
    "# -------------------------\n",
    "from monai.losses import DiceLoss                  # Dice loss for segmentation tasks\n",
    "import segmentation_models_pytorch as smp          # Predefined segmentation architectures (e.g., U-Net)\n",
    "\n",
    "# -------------------------\n",
    "# Training Optimization (AMP)\n",
    "# -------------------------\n",
    "from torch.cuda.amp import autocast, GradScaler    # Automatic mixed precision for faster training\n",
    "\n",
    "# -------------------------\n",
    "# Data Handling & I/O\n",
    "# -------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import cv2\n",
    "import h5py\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from tqdm import tqdm                               # Progress bar for loops\n",
    "\n",
    "# -------------------------\n",
    "# Google Colab Utilities\n",
    "# -------------------------\n",
    "from google.colab import drive                      # Mount Google Drive for I/O\n",
    "from google.colab import files                      # File upload/download support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esELCUUa26My"
   },
   "source": [
    "## 2. Load and Prepare Dataset\n",
    "\n",
    "In this section, we prepare the brain tumor segmentation dataset for model training and validation.\n",
    "\n",
    "The dataset is stored in `.h5` (HDF5) format — a compact and structured format suitable for storing multi-dimensional medical imaging data.  \n",
    "We use standard Python utilities (`glob`, `os`) to recursively search for all `.h5` files within the specified root directory.\n",
    "\n",
    "After collecting all available files, we partition the dataset into training and validation subsets using `train_test_split` from `scikit-learn`.\n",
    "\n",
    "> **Note:** No image or label preprocessing is applied at this stage — only file discovery and path-based dataset splitting are performed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xsz-9ISSto4"
   },
   "source": [
    "### 2.1 Define Dataset Path\n",
    "\n",
    "We define the root directory that contains the dataset files.  \n",
    "Each sample is stored as an `.h5` file — a common format for storing multi-dimensional medical imaging data, such as MRI slices paired with segmentation masks.\n",
    "\n",
    "> **Note:** Update this path according to your environment (e.g., Google Drive, local machine, or cloud storage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUH1qqNf2_SQ"
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/content/drive/MyDrive/your/dataset/path\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2BtZYtsS-RQ"
   },
   "source": [
    "### 2.2 Load `.h5` Files from Dataset Directory\n",
    "\n",
    "We use the `glob` module to recursively search for all `.h5` files within the dataset directory and its subdirectories.\n",
    "\n",
    "> This assumes each `.h5` file stores a single data sample, including both an input MRI slice and its corresponding segmentation mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZL41AkAt5jK8"
   },
   "outputs": [],
   "source": [
    "all_files = glob.glob(os.path.join(dataset_path, \"**\", \"*.h5\"), recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xj2ZTqrdTs5K"
   },
   "source": [
    "### 2.3 Split Dataset into Training and Validation Sets\n",
    "\n",
    "We split the list of `.h5` files into training and validation sets using `train_test_split` from **scikit-learn**.\n",
    "\n",
    "- An 80/20 split is used: 80% for training, 20% for validation.\n",
    "- A fixed `random_state` ensures reproducibility.\n",
    "\n",
    "> Splitting is performed at the file level, with each `.h5` file treated as an independent sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjXSxqfX3oBU"
   },
   "outputs": [],
   "source": [
    "train_path, val_path = train_test_split(all_files, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUZM6Eqg3j7_"
   },
   "source": [
    "## 3. Custom Utilities and Dataset Preparation\n",
    "\n",
    "This section defines the core components of the 2D brain tumor segmentation pipeline.  \n",
    "The implementation is modular to support reusability and easier maintenance:\n",
    "\n",
    "- **Indexing Utilities**: Functions for organizing and indexing dataset content efficiently.\n",
    "- **Custom Dataset Class**: Loads `.h5` files containing MRI slices and segmentation masks using a hybrid patch extraction strategy.\n",
    "- **Data Augmentation**: Albumentations-based pipelines applied to both images and masks.\n",
    "- **Preprocessing Helpers**: Includes mask post-processing and class weight computation to address class imbalance.\n",
    "- **Loss Functions**: Custom loss implementations (e.g., Dice + CrossEntropy) tailored for multiclass medical segmentation.\n",
    "\n",
    "> **Note:** All components are modular and extensible for use in other medical imaging applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnkr-AVc9Bss"
   },
   "source": [
    "### 3.1 Build Index from `.h5` Files\n",
    "\n",
    "We build an index from a list of `.h5` files, where each entry corresponds to a valid (non-empty) image–mask pair.\n",
    "\n",
    "This step improves data loading efficiency by:\n",
    "\n",
    "- Reducing I/O overhead during training\n",
    "- Skipping background-only slices\n",
    "- Enabling optional caching for reuse\n",
    "\n",
    "> **Note:** The core logic is described in the function’s docstring.  \n",
    "This step is recommended for large `.h5`-based datasets to optimize runtime performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3EYP7ao6Go9"
   },
   "outputs": [],
   "source": [
    "def build_index(file_path, save_path=None, filter_empty=True, force_rebuild=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Builds an index of valid (non-empty) slices from a list of `.h5` files.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (List[str]): List of `.h5` file paths to process.\n",
    "        save_path (str, optional): If provided, saves the resulting index as a pickle file.\n",
    "        filter_empty (bool): If True, slices with only background are skipped.\n",
    "        force_rebuild (bool): If True, forces rebuilding the index even if a saved one exists.\n",
    "        verbose (bool): If True, prints progress, skipped files, and error logs.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, int]]: A list of (file_path, slice_index) tuples for valid slices.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "  # Load from existing index if available and rebuild not forced\n",
    "  if save_path and os.path.exists(save_path) and not force_rebuild:\n",
    "    if verbose:\n",
    "      print(f\"[INFO]: Index file found... Loading from {save_path}\")\n",
    "    with open(save_path, 'rb') as f:\n",
    "      return pickle.load(f)\n",
    "\n",
    "  index_list = []\n",
    "  skipped_empty = 0\n",
    "  error_files = 0\n",
    "\n",
    "  if verbose:\n",
    "    print(f\"[INFO]: Building index of {len(file_path)}\")\n",
    "\n",
    "  for path in tqdm(file_path, desc=\"Indexing .h5 files\"):\n",
    "    try:\n",
    "      with h5py.File(path, 'r') as f:\n",
    "        image = f['x'] # shape: (slices, H, W)\n",
    "        mask = f['y'] # shape: (slices, H, W)\n",
    "        for i in range(image.shape[0]):\n",
    "          mask_slice = mask[i]\n",
    "          if filter_empty and mask_slice.sum() == 0:\n",
    "            skipped_empty +=1\n",
    "            continue\n",
    "          index_list.append((path, i))\n",
    "\n",
    "    except Exception as e:\n",
    "      error_files += 1\n",
    "      if verbose:\n",
    "        print(f\"[Warning]: Error file skipped {path} -> {e}\")\n",
    "\n",
    "  if verbose:\n",
    "    print(f\"[INFO]: Total slice valid {len(index_list)}\")\n",
    "    if filter_empty:\n",
    "      print(f\"[INFO]: Total empty slices skipped {skipped_empty}\")\n",
    "    if error_files:\n",
    "      print(f\"[INFO]: Total file errors skipped {error_files}\")\n",
    "\n",
    "  # Save index to pickle file\n",
    "  if save_path:\n",
    "    try:\n",
    "      os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "      with open(save_path, 'wb') as f:\n",
    "        pickle.dump(index_list, f)\n",
    "      if verbose:\n",
    "        print(f\"[INFO]: The index is saved in {save_path} ({len(index_list)} slice)\")\n",
    "    except Exception as e:\n",
    "      print(f\"[Error]: Failed to save index -> {e}\")\n",
    "\n",
    "  return index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5Z0FfGtmdVO"
   },
   "source": [
    "**Example usage**:\n",
    "We call `build_index()` on both the training and validation paths, and save the index as `.pkl` files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PepoX-lGGCIt"
   },
   "outputs": [],
   "source": [
    "save_idx_train_path = \"/content/drive/MyDrive/train_index_dataset.pkl\"\n",
    "save_idx_val_path = \"/content/drive/MyDrive/val_index_dataset.pkl\"\n",
    "\n",
    "train_index = build_index(train_path, save_idx_train_path, force_rebuild=False)\n",
    "val_index = build_index(val_path, save_idx_val_path, force_rebuild=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR-DGgph9J70"
   },
   "source": [
    "### 3.2 Dataset Class: BrainTumorHybrid\n",
    "\n",
    "This custom dataset class is designed to handle `.h5`-based brain tumor segmentation data with hybrid sampling, caching, remapping, normalization, and augmentation. It supports two sampling modes: full slice and random cropped patch, useful for training segmentation models efficiently.\n",
    "\n",
    "- Supports per-epoch subsampling via `max_sample_per_epoch`\n",
    "- Uses patch-based hybrid sampling with control via `hybrid_prob`\n",
    "- Normalizes grayscale image channels and duplicates to 3 channels\n",
    "- Remaps label values from [0, 50, 100, 150] → [0, 1, 2, 3]\n",
    "- Caches image/mask slices for fast loading\n",
    "\n",
    "> **Note:** Caching is automatically enabled when a valid cache_path is provided.\n",
    "If the cache file exists, it will be loaded to reduce repeated disk I/O.\n",
    "Otherwise, the dataset will preload all slices into memory and save the cache for future use.\n",
    "This mechanism can greatly improve performance during training by minimizing disk latency, especially for large datasets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vjq0Tjs568nB"
   },
   "outputs": [],
   "source": [
    "class BrainTumorHybrid(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for 2D brain tumor slices with hybrid sampling and caching.\n",
    "\n",
    "    This dataset supports:\n",
    "    - On-disk caching of preloaded data.\n",
    "    - Hybrid sampling between full-slice and random patch extraction.\n",
    "    - Optional Albumentations transforms.\n",
    "    - Label remapping for tumor segmentation tasks.\n",
    "\n",
    "    Args:\n",
    "        index (List[Tuple[str, int]]): List of (file_path, slice_index) pairs.\n",
    "        cache_path (str, optional): Path to save/load the preloaded cache.\n",
    "        max_sample_per_epoch (int, optional): Maximum number of samples per epoch.\n",
    "        transform (callable, optional): Albumentations transform pipeline.\n",
    "        patch_size (int): Size of square patches to sample.\n",
    "        hybrid_prob (float): Probability of using the full slice instead of patching.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "  # Save the current cache dictionary to disk\n",
    "  def save_cache(self, cache_path):\n",
    "    print(f\"[INFO]: Saved cache to {cache_path}\")\n",
    "    with open(cache_path, 'wb') as f:\n",
    "      pickle.dump(self.cache, f)\n",
    "\n",
    "  # Load cache from disk if exists\n",
    "  def load_cache(self, cache_path):\n",
    "    if os.path.exists(cache_path):\n",
    "      print(f\"[INFO]: Loaded cache from {cache_path}\")\n",
    "      with open(cache_path, 'rb') as f:\n",
    "        self.cache = pickle.load(f)\n",
    "      print(f\"[INFO]: Total cache loaded: {len(self.cache)} slices\")\n",
    "      return True\n",
    "    return False\n",
    "\n",
    "  # Remap original mask labels to consecutive class indices\n",
    "  def remap_labels(self, mask):\n",
    "    remap = {0:0, 50:1, 100:2, 150:3}\n",
    "    return np.vectorize(remap.get)(mask)\n",
    "\n",
    "  # Load & preprocess function\n",
    "  def load_and_preprocess(self, path, i):\n",
    "    with h5py.File(path, 'r') as f:\n",
    "      image = f['x'][i]\n",
    "      mask = f['y'][i]\n",
    "    return image, mask\n",
    "\n",
    "  # Preload cache\n",
    "  def preload_cache(self):\n",
    "    print(f\"[INFO]: Preloading semua data ke cache...\")\n",
    "    for path, i in tqdm(self.full_index, desc=\"Caching Data\"):\n",
    "      key = (path, i)\n",
    "      if key not in self.cache:\n",
    "        image, mask = self.load_and_preprocess(path, i)\n",
    "        self.cache[key] = (image, mask)\n",
    "    print(f\"[INFO]: Total cache: {len(self.cache)} slices\")\n",
    "\n",
    "  # Update index list for each epoch (with optional sampling)\n",
    "  def update_epoch_index(self):\n",
    "    if self.max_sample_per_epoch is None:\n",
    "      self.index = self.full_index\n",
    "    else:\n",
    "      self.index = random.sample(self.full_index, min(self.max_sample_per_epoch, len(self.full_index)))\n",
    "\n",
    "  # Perform random patch extraction (with minimum tumor content)\n",
    "  def random_crop(self, image, mask):\n",
    "    H, W = image.shape[:2]\n",
    "    ps = self.patch_size\n",
    "    if H < ps or W < ps:\n",
    "      return image, mask\n",
    "\n",
    "    for _ in range(10):\n",
    "      x = np.random.randint(0, W - ps)\n",
    "      y = np.random.randint(0, H - ps)\n",
    "      img_patch = image[y:y+ps, x:x+ps]\n",
    "      mask_patch = mask[y:y+ps, x:x+ps]\n",
    "\n",
    "      if np.sum(mask_patch > 0) > 10:\n",
    "        img_patch = cv2.resize(img_patch, (240, 240), interpolation=cv2.INTER_LINEAR)\n",
    "        mask_patch = cv2.resize(mask_patch, (240, 240), interpolation=cv2.INTER_NEAREST)\n",
    "        return img_patch, mask_patch\n",
    "\n",
    "    img_patch = image[0:ps, 0:ps]\n",
    "    mask_patch = mask[0:ps, 0:ps]\n",
    "    img_patch = cv2.resize(img_patch, (240, 240), interpolation=cv2.INTER_LINEAR)\n",
    "    mask_patch = cv2.resize(mask_patch, (240, 240), interpolation=cv2.INTER_NEAREST)\n",
    "    return img_patch, mask_patch\n",
    "\n",
    "  # Init Datset\n",
    "  def __init__(self, index, cache_path=None, max_sample_per_epoch=None, transform=None, patch_size=160, hybrid_prob=0.3):\n",
    "    self.full_index = index\n",
    "    self.cache_path = cache_path\n",
    "    self.cache = dict()\n",
    "    self.max_sample_per_epoch = max_sample_per_epoch\n",
    "    self.transform = transform\n",
    "    self.patch_size = patch_size # Patch size used in random cropping\n",
    "    self.hybrid_prob = hybrid_prob # Probability of using full slice (vs patch)\n",
    "\n",
    "    if cache_path:\n",
    "      if not self.load_cache(cache_path):\n",
    "        self.preload_cache()\n",
    "        self.save_cache(cache_path)\n",
    "\n",
    "    self.update_epoch_index()\n",
    "\n",
    "  # Return dataset length (after sampling)\n",
    "  def __len__(self):\n",
    "    return len(self.index)\n",
    "\n",
    "  # Retrieve a single sample from the dataset\n",
    "  def __getitem__(self, idx):\n",
    "    path, i = self.index[idx]\n",
    "    key = (path, i)\n",
    "\n",
    "    # Retrieve from cache if available, otherwise load from file\n",
    "    if key in self.cache:\n",
    "      image, mask = self.cache[key]\n",
    "    else:\n",
    "      image, mask = self.load_and_preprocess(path, i)\n",
    "\n",
    "    # Remap mask\n",
    "    mask = self.remap_labels(mask).astype(np.int64)\n",
    "\n",
    "    # Resize early to ensure consistent shape before augmentation\n",
    "    image = cv2.resize(image, (240, 240), interpolation=cv2.INTER_LINEAR)\n",
    "    mask = cv2.resize(mask, (240, 240), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Normalize image using z-score normalization\n",
    "    image = image.astype(np.float32)\n",
    "    image = (image - image.mean()) / (image.std() + 1e-5)\n",
    "\n",
    "    # Expand grayscale image to 3-channel to match pretrained model expectations\n",
    "    image = np.expand_dims(image, axis=-1)\n",
    "    image = np.repeat(image, 3, axis=-1)\n",
    "\n",
    "    # Hybrid sampling: randomly choose between full slice or patch\n",
    "    if random.random() < self.hybrid_prob:\n",
    "      img_patch, mask_patch = image, mask\n",
    "    else:\n",
    "      img_patch, mask_patch = self.random_crop(image, mask)\n",
    "\n",
    "    # Apply augmentation if provided\n",
    "    if self.transform:\n",
    "      augmentation = self.transform(image=img_patch, mask=mask_patch)\n",
    "      img_patch = augmentation['image']\n",
    "      mask_patch = augmentation['mask']\n",
    "\n",
    "      mask_patch = mask_patch.squeeze(0) if mask_patch.ndim == 2 else mask_patch\n",
    "      mask_patch = mask_patch.long()\n",
    "\n",
    "    return img_patch, mask_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTWXgscOqqUl"
   },
   "source": [
    "### 3.3 Data Augmentation Pipelines\n",
    "\n",
    "This section defines the data augmentation strategies applied during training and validation.  \n",
    "Augmentation is crucial in medical image segmentation to increase model robustness by simulating real-world variability.\n",
    "\n",
    "We use **Albumentations** to construct flexible and composable pipelines:\n",
    "\n",
    "- **`train_T`** — Includes both geometric (e.g., flips, rotations) and intensity-based augmentations (e.g., brightness, contrast) to promote generalization and prevent overfitting.\n",
    "- **`val_T`** — Applies only resizing and normalization to ensure consistency during validation.\n",
    "\n",
    "> **Note:** These augmentations are applied on-the-fly within the dataset class, ensuring efficient and randomized augmentation for each training epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huwukPJzCxjM"
   },
   "outputs": [],
   "source": [
    "# Transform for training data (with augmentation)\n",
    "train_T = A.Compose([\n",
    "    A.Resize(240, 240),                       # Resize input to 240x240\n",
    "    A.GridDistortion(p=0.2),                  # Apply grid distortion\n",
    "    A.RandomBrightnessContrast(p=0.3),        # Adjust brightness and contrast\n",
    "    A.GaussNoise(p=0.2),                      # Add Gaussian noise\n",
    "    A.HorizontalFlip(p=0.3),                  # Random horizontal flip\n",
    "    A.VerticalFlip(p=0.3),                    # Random vertical flip\n",
    "    A.ToTensorV2()                            # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "# Transform for validation data (no augmentation)\n",
    "val_T = A.Compose([\n",
    "    A.Resize(240, 240),\n",
    "    A.ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmdZzI5mpLn1"
   },
   "source": [
    "### 3.4 Data Preparation and Loading\n",
    "\n",
    "We instantiate the custom `BrainTumorHybrid` dataset for both training and validation phases, configuring caching, transformation, and sampling strategies.\n",
    "\n",
    "- **Training Set**  \n",
    "  Uses *hybrid sampling* (a mix of full slices and random patches) to increase sample diversity and focus on lesion regions.  \n",
    "  Augmentations are applied on-the-fly using the `train_T` pipeline.\n",
    "\n",
    "- **Validation Set**  \n",
    "  Uses *only full slices* by setting `hybrid_prob=1.0`, ensuring that patch sampling is disabled for fair evaluation.  \n",
    "  Augmentations are minimal (`val_T`) to maintain consistency.\n",
    "\n",
    "We also define **PyTorch DataLoaders** with appropriate batch sizes and multithreading for efficient data streaming during training.\n",
    "\n",
    "> **Note:** Dataset caching is automatically handled — if a cache file exists, it will be loaded; otherwise, it will be generated from scratch and saved for future runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9eMxTBKGPt9"
   },
   "outputs": [],
   "source": [
    "# Cache path\n",
    "cache_train_dataset_path = \"/content/drive/MyDrive/cache_train_dataset.pkl\"\n",
    "cache_val_dataset_path = \"/content/drive/MyDrive/cache_val_dataset.pkl\"\n",
    "\n",
    "# Instantiate datasets\n",
    "train_dataset = BrainTumorHybrid(\n",
    "    train_index,\n",
    "    transform=train_T,\n",
    "    cache_path=cache_train_dataset_path\n",
    ")\n",
    "\n",
    "val_dataset = BrainTumorHybrid(\n",
    "    val_index,\n",
    "    transform=val_T,\n",
    "    hybrid_prob=1.0,\n",
    "    cache_path=cache_val_dataset_path\n",
    ")\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1Bv0prV9UQU"
   },
   "source": [
    "### 3.5 Class Weights Computation\n",
    "\n",
    "To address the issue of **class imbalance** in the segmentation masks, we compute class weights based on the **pixel distribution** in the training dataset.  \n",
    "These weights are especially useful when using loss functions such as `CrossEntropyLoss` that support per-class weighting via the `weight` argument.\n",
    "\n",
    "- **`collect_mask_from_index`**  \n",
    "  Extracts flattened segmentation masks from the full-slice training dataset.  \n",
    "  This step bypasses any patch sampling or data augmentations to reflect the true class distribution.\n",
    "\n",
    "- **`calculate_class_weight`**  \n",
    "  Computes class weights using `sklearn.utils.class_weight.compute_class_weight` and saves the result as a PyTorch tensor in `.pth` format for reuse.\n",
    "\n",
    "> **Note:** Accurate class weights can improve training stability and performance on underrepresented classes (e.g., necrotic core).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVj7UcXEBhsl"
   },
   "outputs": [],
   "source": [
    "# Collect mask form index\n",
    "def collect_mask_from_index(index):\n",
    "    \"\"\"\n",
    "    Collects masks directly from the HDF5 files using the provided index.\n",
    "\n",
    "    This bypasses augmentations and patch sampling done in the Dataset class.\n",
    "    The mask labels are remapped to class indices (0-3) and flattened for use\n",
    "    in class weight computation.\n",
    "\n",
    "    Args:\n",
    "        index (List[Tuple[str, int]]): List of (file_path, slice_index) pairs.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: Flattened list of remapped mask values.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "  all_labels = []\n",
    "\n",
    "  # Loop through each indexed file and slice\n",
    "  for path, i in tqdm(index, desc=\"Collecting mask\"):\n",
    "    with h5py.File(path, 'r') as f:\n",
    "      mask = f['y'][i]\n",
    "\n",
    "      # Remap original pixel values to class indices\n",
    "      remap = {0: 0, 50: 1, 100: 2, 150: 3}\n",
    "      mask = np.vectorize(remap.get)(mask)\n",
    "\n",
    "      # Flatten and collect all label values\n",
    "      all_labels.extend(mask.flatten())\n",
    "  return all_labels\n",
    "\n",
    "# Compute class weight\n",
    "def calculate_class_weight(index, num_classes=4, save_path=None, load_if_available=True):\n",
    "      \"\"\"\n",
    "    Calculates class weights from the ground-truth masks for use in loss balancing.\n",
    "\n",
    "    This function computes balanced class weights based on the frequency of each class\n",
    "    label found in the dataset. It supports saving and loading from a cache file.\n",
    "\n",
    "    Args:\n",
    "        index (List[Tuple[str, int]]): List of (file_path, slice_index) to extract masks from.\n",
    "        num_classes (int): Number of classes to consider for weight computation.\n",
    "        save_path (str, optional): Path to save the computed weights.\n",
    "        load_if_available (bool): If True, loads precomputed weights if available.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Class weights as a float tensor of shape (num_classes,).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "  # Load from cache if available\n",
    "  if load_if_available and save_path and os.path.exists(save_path):\n",
    "     print(f\"[INFO]: Index found... Loading from {save_path}\")\n",
    "     return torch.load(save_path)\n",
    "\n",
    "  # Collect flattened ground-truth labels from the dataset\n",
    "  labels = collect_mask_from_index(index)\n",
    "\n",
    "  # Compute class weights using scikit-learn\n",
    "  weight = compute_class_weight('balanced', classes=np.arange(num_classes), y=labels)\n",
    "  weight_tensor = torch.tensor(weight, dtype=torch.float32)\n",
    "\n",
    "  # Save to cache if needed\n",
    "  if save_path:\n",
    "    torch.save(weight_tensor, save_path)\n",
    "    print(f\"[INFO]: Class weight file saved to {save_path}\")\n",
    "\n",
    "  return weight_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FkOS_FZ7w4x"
   },
   "outputs": [],
   "source": [
    "save_path_weight_calculate = \"/content/drive/MyDrive/weight_calculate.pth\"\n",
    "\n",
    "# call function calculate_class_weight\n",
    "class_weight = calculate_class_weight(\n",
    "    train_index,\n",
    "    num_classes=4,\n",
    "    save_path=save_path_weight_calculate,\n",
    "    load_if_available=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RgFdBlJ9_ZQ"
   },
   "source": [
    "### 3.6 Combined Loss Function (CrossEntropy + Dice)\n",
    "\n",
    "To effectively train the segmentation model on imbalanced medical data, we define a custom loss function that combines **CrossEntropyLoss** (pixel-wise classification) and **DiceLoss** (region-based overlap measure).  \n",
    "This hybrid approach balances *local accuracy* (via CrossEntropy) and *global structure preservation* (via Dice), which is critical for robust tumor segmentation.\n",
    "\n",
    "> **Note:** DiceLoss helps the model better capture smaller or less frequent tumor subregions, which are often underrepresented in the dataset.\n",
    "\n",
    "The `ComboLoss` class internally handles reshaping and formatting, making it compatible with both **MONAI** and native **PyTorch** segmentation pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQcXFOtLD_LE"
   },
   "outputs": [],
   "source": [
    "# Combined Loss: CrossEntropyLoss + DiceLoss\n",
    "class ComboLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines CrossEntropyLoss and DiceLoss for multi-class segmentation.\n",
    "\n",
    "    Args:\n",
    "        ce_weight (Tensor or None): Optional tensor of class weights for CrossEntropyLoss.\n",
    "        dice_weight (float): Scaling factor for DiceLoss. Default is 1.0.\n",
    "        ce_scale (float): Scaling factor for CrossEntropyLoss. Default is 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "  def __init__(self, ce_weight=None, dice_weight=1.0, ce_scale=1.0): # ce=CrossEntropy\n",
    "    super().__init__()\n",
    "    self.ce = nn.CrossEntropyLoss(weight=ce_weight) # CE Defined\n",
    "    self.dice = DiceLoss(to_onehot_y=True, softmax=True) # DL Defined\n",
    "    self.dice_weight = dice_weight\n",
    "    self.ce_scale = ce_scale\n",
    "\n",
    "  def forward(self, preds, targets):\n",
    "        \"\"\"\n",
    "        Computes the combined loss.\n",
    "\n",
    "        Args:\n",
    "            preds (Tensor): Predicted logits of shape (B, C, H, W).\n",
    "            targets (Tensor): Ground truth mask of shape (B, H, W) or (B, 1, H, W).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Combined loss value.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    # Prepare targets for CrossEntropyLoss\n",
    "    targets_ce = targets.squeeze(1) if targets.ndim == 4 else targets\n",
    "\n",
    "    # Prepare targets for DiceLoss\n",
    "    targets_dice = targets.unsqueeze(1) if targets.ndim == 3 else targets\n",
    "\n",
    "    # Compute individual loss components\n",
    "    loss_ce = self.ce(preds, targets_ce)\n",
    "    loss_dice = self.dice(preds, targets_dice)\n",
    "\n",
    "    # Return combined weighted loss\n",
    "    return self.ce_scale * loss_ce + self.dice_weight * loss_dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAwQQKRcJ2ZT"
   },
   "source": [
    "## 4. Model Initialization & Training Configuration\n",
    "\n",
    "This section defines the segmentation model architecture and sets up the full training configuration pipeline.  \n",
    "The model leverages a pre-trained encoder for efficiency, and all components are organized for clarity and reproducibility.\n",
    "\n",
    "> **Note:** `EfficientNet-B1` is selected as the encoder due to its strong balance between accuracy and computational efficiency. It is initialized with ImageNet weights.\n",
    "\n",
    "**Key components:**\n",
    "- **Model**: U-Net-style architecture with an `EfficientNet-B1` encoder backbone.\n",
    "- **Loss Function**: Combination of `CrossEntropyLoss` and `DiceLoss`, designed to handle class imbalance and spatial overlap effectively.\n",
    "- **Optimizer**: Adam with weight decay for regularization.\n",
    "- **Learning Rate Scheduler**: `ReduceLROnPlateau` to adaptively reduce the learning rate based on validation loss.\n",
    "- **Early Stopping**: Monitors validation loss to prevent overfitting.\n",
    "- **Metric**: Multiclass Jaccard Index (IoU) for evaluating segmentation quality across all classes.\n",
    "- **AMP (Automatic Mixed Precision)**: Enabled via `GradScaler` for faster training and reduced memory usage on compatible GPUs.\n",
    "\n",
    "All configurations are defined prior to training to ensure clarity, reproducibility, and compatibility with different hardware environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wx2Gjhkz0Ip"
   },
   "source": [
    "### 4.1 Device Setup and Model Initialization\n",
    "\n",
    "We begin by defining the computation device (GPU or CPU), and then initialize the segmentation model.  \n",
    "The model follows a U-Net architecture with an `EfficientNet-B1` encoder, pre-trained on ImageNet for better feature extraction and faster convergence.\n",
    "\n",
    "To accelerate training and reduce overfitting during early epochs, all encoder layers are initially frozen.\n",
    "\n",
    "> **Note:** Freezing the encoder enables the decoder to learn effectively at the start. Fine-tuning can be performed later by unfreezing the encoder layers once the decoder has stabilized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rs8HMk7FKsPP"
   },
   "outputs": [],
   "source": [
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load model from smp as Encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name='efficientnet-b1',\n",
    "    encoder_weights='imagenet',\n",
    "    in_channels=3,\n",
    "    classes=4\n",
    ").to(device)\n",
    "\n",
    "# Freeze encoder layers\n",
    "for param in model.encoder.parameters():\n",
    "  param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2kB3Qu5HMku"
   },
   "source": [
    "### 4.2 Early Stopping\n",
    "\n",
    "To avoid overfitting and reduce unnecessary computation, we define a custom **EarlyStopping** utility.  \n",
    "This mechanism monitors the validation metric (e.g., loss or accuracy) and halts training if no improvement is observed over a defined patience period.\n",
    "\n",
    "It supports both `'min'` and `'max'` monitoring modes, depending on whether the target metric is expected to decrease (e.g., loss) or increase (e.g., accuracy or IoU).\n",
    "\n",
    "> **Note:** Early stopping is particularly useful when training high-capacity models on limited datasets, ensuring training stops at the optimal point before performance degrades.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ged-mw83Nsu7"
   },
   "outputs": [],
   "source": [
    "# Custom early stopping class to monitor validation performance\n",
    "# Stops training if no improvement is observed over 'patience' epochs\n",
    "class EarlyStopping:\n",
    "    def __init__(self, monitor='val_loss', mode='min', patience=3, delta=0.0, verbose=True):\n",
    "         \"\"\"\n",
    "        Args:\n",
    "            monitor (str): Metric to monitor ('val_loss' or 'val_acc')\n",
    "            mode (str): 'min' → lower is better, 'max' → higher is better\n",
    "            patience (int): # of epochs with no improvement before stopping\n",
    "            delta (float): Minimum change to qualify as improvement\n",
    "            verbose (bool): Print status each epoch if True\n",
    "        \"\"\"\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "        # Set comparison function and initial best value\n",
    "        if self.mode == 'min':\n",
    "            self.monitor_op = lambda current, best: current < best - self.delta\n",
    "            self.best_score = np.inf\n",
    "        elif self.mode == 'max':\n",
    "            self.monitor_op = lambda current, best: current > best + self.delta\n",
    "            self.best_score = -np.inf\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'min' or 'max'\")\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        # Initialize best score\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] Initial best {self.monitor}: {self.best_score:.4f}\")\n",
    "        # Check for improvement\n",
    "        elif self.monitor_op(current_score, self.best_score):\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] Improved {self.monitor}: {self.best_score:.4f}\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] No improvement in {self.monitor} for {self.counter}/{self.patience} epochs.\")\n",
    "            # Stop training if performance has not improved for 'patience' epochs\n",
    "            if self.counter >= self.patience:\n",
    "                if self.verbose:\n",
    "                    print(f\"[EarlyStopping] Stopping training. Best {self.monitor}: {self.best_score:.4f}\")\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02lUIAmO3ylg"
   },
   "source": [
    "#### 4.2.1 EarlyStopping Configuration\n",
    "\n",
    "After defining the `EarlyStopping` class, we instantiate it by specifying:\n",
    "\n",
    "- `monitor`: The validation metric to observe (`val_loss`)\n",
    "- `mode`: Whether to minimize or maximize the metric (`'min'` for loss)\n",
    "- `patience`: The number of consecutive epochs to wait without improvement\n",
    "\n",
    "This configuration ensures that training is terminated when the model stops improving, thereby avoiding overfitting and reducing computational cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PC-uKONTN6rw"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    patience=3,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCcf87_F3hIW"
   },
   "source": [
    "### 4.3 Loss Function\n",
    "\n",
    "We define a composite loss function by combining **CrossEntropyLoss** and **DiceLoss**, which is particularly suitable for multi-class medical image segmentation tasks characterized by class imbalance and irregular anatomical structures.\n",
    "\n",
    "**Key characteristics:**\n",
    "- **CrossEntropyLoss** handles pixel-wise classification and supports class weights to account for imbalanced label distributions.\n",
    "- **DiceLoss** emphasizes spatial overlap between prediction and ground truth, making it effective for capturing small or sparse tumor regions.\n",
    "\n",
    "The custom `ComboLoss` implementation integrates both components and automatically moves the loss function to the appropriate computation device (`CPU` or `GPU`) for efficient training.\n",
    "\n",
    "> **Note:** This hybrid loss function is designed to balance region-based similarity and classification accuracy, providing more stable and meaningful learning signals across all tumor classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB9kCa7aJGUR"
   },
   "outputs": [],
   "source": [
    "criterion = ComboLoss(ce_weight=class_weight).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6HBfUp-5sWC"
   },
   "source": [
    "### 4.4 Optimizer Configuration\n",
    "\n",
    "We configure the optimizer using **Adam**, known for its adaptive learning rate and fast convergence in deep learning tasks.  \n",
    "A base learning rate of `1e-3` is selected, which works well as a starting point for most segmentation models.\n",
    "\n",
    "> **Note:** Weight decay can be applied through `Adam` to introduce L2 regularization, helping reduce overfitting in deeper networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEvmnXc_JUtZ"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIH4cNlE6TmS"
   },
   "source": [
    "### 4.5 Learning Rate Scheduling\n",
    "\n",
    "To enable dynamic learning rate adjustment during training, we configure **ReduceLROnPlateau**, a scheduler that monitors the validation loss and reduces the learning rate when no significant improvement is observed.\n",
    "\n",
    "**Key characteristics:**\n",
    "- **Mode**: `'min'` — activates when the validation loss stops decreasing.\n",
    "- **Factor**: `0.1` — reduces the learning rate by a factor of 10.\n",
    "- **Patience**: `2` — waits for two stagnant epochs before reducing.\n",
    "- **Verbose**: Enabled to log learning rate changes.\n",
    "\n",
    "> **Note:** This scheduler is particularly suitable for medical segmentation tasks, where validation loss may fluctuate due to class imbalance or noisy annotations.  \n",
    "> Adaptive scheduling allows the optimizer to take larger steps when improving and smaller steps when plateauing, enhancing convergence stability and helping avoid local minima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9RGVb24NUxL"
   },
   "outputs": [],
   "source": [
    "scheduler = ReduceLROnPlateau(\n",
    "    early_optimaizer,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AT23hr4w6tIn"
   },
   "source": [
    "### 4.6 Evaluation Metric\n",
    "\n",
    "For segmentation performance evaluation, we use the **Multiclass Jaccard Index** (also known as Intersection over Union, IoU). This metric is particularly effective for multi-class problems, as it measures the overlap between predicted and ground truth masks across all classes.\n",
    "\n",
    "The Jaccard Index is computed as:\n",
    "\n",
    "$$\n",
    "IoU = \\frac{TP}{TP + FP + FN}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **TP**: True Positives\n",
    "- **FP**: False Positives\n",
    "- **FN**: False Negatives\n",
    "\n",
    "This metric provides a more interpretable performance indicator than accuracy in segmentation tasks, especially when class imbalance is present.\n",
    "\n",
    "> **Note:** The Multiclass Jaccard Index is particularly sensitive to class imbalance and is widely used in medical image segmentation benchmarks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwV0shIizwck"
   },
   "outputs": [],
   "source": [
    "matric = MulticlassJaccardIndex(num_classes=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omK0tiDiBq_8"
   },
   "source": [
    "### 4.7 Mixed Precision Training Setup\n",
    "\n",
    "To accelerate training and reduce GPU memory usage, we enable **Automatic Mixed Precision (AMP)** using PyTorch’s `GradScaler`. AMP performs selected operations in half precision while maintaining stability via dynamic gradient scaling, making it well-suited for large models and high-resolution data.\n",
    "\n",
    "> **Note:** While AMP can significantly improve performance, it may introduce instability in certain architectures or loss functions. Careful monitoring during training is recommended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAoWMXdGB8G6"
   },
   "outputs": [],
   "source": [
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uN0EmtZrOCbt"
   },
   "source": [
    "## 5. Model Training\n",
    "\n",
    "This section implements the training loop for our segmentation model, incorporating **Automatic Mixed Precision (AMP)** for computational efficiency and GPU memory optimization. The training is guided by both loss and Intersection over Union (IoU) metrics on training and validation sets, with **early stopping** and **learning rate scheduling** to enhance convergence and avoid overfitting.\n",
    "\n",
    "**Key components:**\n",
    "\n",
    "- **Automatic Mixed Precision (AMP):** Accelerates training by using float16 where appropriate, while preserving numerical stability via `GradScaler`.\n",
    "- **IoU Metric:** Evaluates segmentation performance more robustly than accuracy.\n",
    "- **Early Stopping:** Terminates training when validation loss no longer improves after a patience threshold.\n",
    "- **ReduceLROnPlateau Scheduler:** Dynamically lowers the learning rate when performance stagnates.\n",
    "\n",
    "> **Note:** Maintain the correct order of operations (`forward → backward → scaler.step(optimizer) → zero_grad()`) when using AMP. Incorrect sequencing may lead to unstable gradients or halted learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgPc1BtgKPg8"
   },
   "source": [
    "### 5.1 Training Loop\n",
    "\n",
    "We define the full training loop across a fixed number of epochs. Each epoch is composed of two distinct phases: **training** and **validation**.\n",
    "\n",
    "- During the **training phase**, the model performs a forward and backward pass using **Automatic Mixed Precision (AMP)** with dynamic gradient scaling (`GradScaler`).\n",
    "- In the **validation phase**, the model is evaluated without gradient updates to track performance metrics.\n",
    "\n",
    "After each validation:\n",
    "- The learning rate scheduler (`ReduceLROnPlateau`) is updated based on validation loss.\n",
    "- **Early stopping** checks whether the validation loss has improved.\n",
    "- If the current model achieves a higher validation IoU, its weights are stored in memory and saved to disk.\n",
    "\n",
    "> **Note:** Always ensure that gradient scaling, optimizer stepping, and zeroing are correctly ordered. When using AMP, an incorrect sequence may silently result in no learning progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRQvdtN4OFyg"
   },
   "outputs": [],
   "source": [
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_val_iou = 0.0\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  print(\"-\" * 50)\n",
    "  print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "  print(\"-\" * 50)\n",
    "\n",
    "  # Optional: dynamically update training set (e.g. patch-based sampling)\n",
    "  # If applicable, reinitialize DataLoader per epoch to reflect updated sampling\n",
    "  # -------------------------------------------------------------------------------------------------------------\n",
    "  # train_dataset.update_epoch_index()\n",
    "  # train_loader = DataLoader(train_dataset, batch_size=x, shuffle=True, num_workers=4, pin_memory=True)\n",
    "  # -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "  # --- Train Phases ---\n",
    "  model.train()\n",
    "  train_loss, train_iou = 0.0, 0.0\n",
    "\n",
    "  for image, mask in tqdm(train_loader, desc=\"Training\"):\n",
    "    image, mask = image.to(device), mask.to(device)\n",
    "\n",
    "    # Forward pass with mixed precision (autocast increases speed and reduces memory usage)\n",
    "    with autocast():\n",
    "      outputs = model(image)\n",
    "      loss = criterion(outputs, mask)\n",
    "\n",
    "    # Clear gradients to prevent accumulation in the next iteration\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass with scaled gradients (helps prevent underflow in float16 training)\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    # Update model parameters using scaled gradients\n",
    "    scaler.step(optimizer)\n",
    "\n",
    "    # Update the scaler for dynamic loss scaling\n",
    "    scaler.update()\n",
    "\n",
    "    # Track total loss and IoU for monitoring training progress\n",
    "    train_loss += loss.item() * image.size(0)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    train_iou += matric(preds, mask).item() * image.size(0)\n",
    "\n",
    "  avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "  avg_train_iou = train_iou / len(train_loader.dataset)\n",
    "\n",
    "  # --- Val Phases ---\n",
    "  model.eval()\n",
    "  val_loss, val_iou = 0.0, 0.0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for image, mask in tqdm(val_loader, desc=\"Validation\"):\n",
    "      image, mask = image.to(device), mask.to(device)\n",
    "\n",
    "      outputs = model(image)\n",
    "      loss = criterion(outputs, mask)\n",
    "\n",
    "      val_loss += loss.item() * image.size(0)\n",
    "      preds = torch.argmax(outputs, dim=1)\n",
    "      val_iou += matric(preds, mask).item() * image.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    avg_val_iou = val_iou / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Train IoU: {avg_train_iou:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f}     | Val IoU: {avg_val_iou:.4f}\")\n",
    "\n",
    "    # Adjust learning rate based on validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Check early stopping condition\n",
    "    early_stopping(avg_val_loss)\n",
    "\n",
    "    # Save best model based on validation IoU\n",
    "    if avg_val_iou > best_val_iou:\n",
    "      best_val_iou = avg_val_iou\n",
    "      best_model_wts = copy.deepcopy(model.state_dict())\n",
    "      torch.save(model.state_dict(), \"best_warmup_segmentation_model.pth\")\n",
    "      print(f\"[INFO]: New Model Saved!\")\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "      print(\"[INFO]: Early Stop Trigred!\")\n",
    "      break\n",
    "\n",
    "model.load_state_dict(best_model_wts)\n",
    "print(\"Best Model Earlytrain Loaded!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6Z0sn97KeJG"
   },
   "source": [
    "### 5.2 Saving the Best Model\n",
    "\n",
    "The model weights corresponding to the **highest validation IoU** are stored in memory (`best_model_wts`) during training. This ensures that we retain the most performant version of the model regardless of when training ends (either by early stopping or epoch limit).\n",
    "\n",
    "At the end of training:\n",
    "- The model is reloaded with these best weights.\n",
    "- The weights are then saved to disk for later evaluation or deployment.\n",
    "\n",
    "> **Note:** Reloading the best weights before saving ensures that the final model reflects the best validation performance achieved during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQc9kTwz73v9"
   },
   "outputs": [],
   "source": [
    "# Save the best model to disk for future use (e.g., inference or further evaluation)\n",
    "torch.save(model.state_dict(), \"/content/drive/MyDrive/braTS_earlyTrain_segModel_01.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cuiUWRygicr"
   },
   "source": [
    "# Conclusion & Reflection\n",
    "\n",
    "This notebook represents a focused exploration into 2D brain tumor segmentation using deep learning — developed as a personal learning project and portfolio showcase.\n",
    "\n",
    "Through this process, I designed and implemented a modular pipeline that includes dataset preprocessing, class imbalance handling, mixed-precision training (AMP), metric evaluation with IoU, and best-model saving strategies. Every stage was crafted with reproducibility and clarity in mind, helping me deepen my understanding of practical model development in the context of medical image segmentation.\n",
    "\n",
    "This work not only enhanced my technical skills in PyTorch and segmentation workflows, but also trained my ability to think structurally — planning each section of the notebook to build toward a cohesive and explainable pipeline. Additionally, it highlighted key challenges in medical AI, such as class imbalance and efficient resource usage.\n",
    "\n",
    ">💡 As part of an ongoing journey, this notebook lays the foundation for future work involving 3D volumetric segmentation, explainable AI (XAI), and deployment through interactive tools — gradually advancing toward more robust and human-centric AI systems in medical imaging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEgUu2gmuyVc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMJZ+voohojennOgIe0H57S",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
