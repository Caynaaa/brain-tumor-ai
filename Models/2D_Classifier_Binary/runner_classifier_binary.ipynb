{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Brain Tumor Classifier — Runner Notebook\n",
        "\n",
        "> **Purpose:** This notebook serves as the execution script for the **binary brain tumor classification model**.  \n",
        "> All core components (model architecture, data processing, training pipeline) are defined in separate Python modules to maintain a clean and modular project structure.\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook is part of the **Brain Tumor AI** project, focusing on **binary classification** of medical images (tumor vs. no tumor).  \n",
        "It is designed to:\n",
        "- Load and configure the modular components (model, data module, transforms, helpers, callbacks, loggers).\n",
        "- Execute the training process using **PyTorch Lightning**.\n",
        "- Save the trained model for inference.\n",
        "\n",
        "By separating logic into `.py` files, the project ensures:\n",
        "- **Reusability:** Components can be reused across multiple experiments.\n",
        "- **Maintainability:** Easier debugging and updates.\n",
        "- **Clarity:** The notebook focuses on workflow and results, not implementation details.\n",
        "\n",
        "\n",
        "> **Note:** This project is for learning and portfolio purposes only — not for clinical use.\n"
      ],
      "metadata": {
        "id": "0fRXFKijRA-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies & Import Libraries"
      ],
      "metadata": {
        "id": "TnOSLKuepLbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Install Dependencies\n",
        "Install the required packages to ensure the notebook runs without missing dependencies.\n",
        "\n",
        "- **`datasets`** — Dataset handling and loading utilities.  \n",
        "- **`fsspec`** — File system interface for remote/local storage.  \n",
        "- **`pytorch-lightning`** — High-level PyTorch framework for training.  \n",
        "- **`tensorboard`** — Visualization of training logs.  \n",
        "- **`albumentations`** — Advanced image augmentation library.  \n",
        "- **`torchmetrics`** — Standardized metrics for PyTorch.\n",
        "\n",
        "> Skip this step if the environment already has these packages installed.\n"
      ],
      "metadata": {
        "id": "dFYDbhEOpT5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U datasets fsspec pytorch-lightning tensorboard albumentations torchmetrics"
      ],
      "metadata": {
        "id": "GLwWq8vtRyIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Import Required Libraries\n",
        "\n",
        "Below are the required libraries and modules used in this notebook:\n",
        "\n",
        "- **os, sys** — For file and system path handling.\n",
        "- **torch** — PyTorch core library for deep learning operations.\n",
        "- **pytorch_lightning** — High-level wrapper for PyTorch to simplify training loops.\n",
        "- **TensorBoardLogger** — For logging training metrics to TensorBoard.\n",
        "- **scikit-learn (train_test_split, compute_class_weight)** — For dataset splitting and class weight computation.\n",
        "- **google.colab.drive** — To mount Google Drive and access stored datasets/models.\n",
        "- **datasets.load_dataset** — To load datasets in various formats from the Hugging Face Datasets library.\n"
      ],
      "metadata": {
        "id": "jwD7Cn5TWl5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "ZKnczkRAWmPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define Project paths and Import Custom Module"
      ],
      "metadata": {
        "id": "J5rVVgyUR0ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Configure Directory Paths  \n",
        "Here, we define the key directory paths used throughout the project:  \n",
        "\n",
        "- **`CHECKPOINT_PATH`** — Location where model checkpoints will be saved and loaded from.  \n",
        "- **`PROJECT_PATH`** — Root path of the project, used as a base reference for file operations.  \n",
        "- **`SAVE_PATH`** — Directory for storing final outputs, such as trained models.  \n",
        "\n",
        "The `PROJECT_PATH` is appended to `sys.path` to make sure Python can locate and import the project modules without issues.\n"
      ],
      "metadata": {
        "id": "72aekfewsHv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH = \"\"\n",
        "\n",
        "PROJECT_PATH = \"\"\n",
        "\n",
        "SAVE_PATH = \"\""
      ],
      "metadata": {
        "id": "wVKxmzy09MzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if PROJECT_PATH not in sys.path:\n",
        "    sys.path.append(PROJECT_PATH)"
      ],
      "metadata": {
        "id": "-JcDJW7pWgkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Import Custom Modules\n",
        "\n",
        "This section imports the custom Python modules that define the model architecture, data pipeline, training callbacks, and helper functions.  \n",
        "By keeping these components in separate files, the project maintains a clean and modular structure.\n",
        "\n",
        "- **DenseNetClassifierBinary** → Custom PyTorch Lightning model for binary brain tumor classification.  \n",
        "- **BrainTumorDataModule** → Handles data loading, preprocessing, and batching using PyTorch Lightning's DataModule structure.  \n",
        "- **get_callbacks** → Retrieves predefined training callbacks such as model checkpointing and early stopping.  \n",
        "- **set_seed** → Utility function to ensure reproducibility across runs.\n"
      ],
      "metadata": {
        "id": "nxinANrqWg_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from module import DenseNetClassifierBinary\n",
        "from datamodule import BrainTumorDataModule\n",
        "from callbacks import get_callbacks\n",
        "from helper import set_seed"
      ],
      "metadata": {
        "id": "IDfS0VOZRBO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define Seed, Load and Prepare Raw Dataset\n"
      ],
      "metadata": {
        "id": "YiCtiaVZSx_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Set Random Seed\n",
        "\n",
        "To ensure reproducibility of results, a fixed random seed is set at the beginning of the data preparation process.  \n",
        "By setting the seed, all operations involving randomness (such as data shuffling, train-test splitting, and weight initialization) will produce the same outcome each time the notebook is executed. This step is crucial for debugging and for achieving consistent experimental results.\n"
      ],
      "metadata": {
        "id": "EesbwTvnZINp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)"
      ],
      "metadata": {
        "id": "-gOuI7hPZE29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Load Dataset from Hugging Face\n",
        "\n",
        "We load the dataset directly using the `datasets` library. The dataset contains labeled 2D brain MRI scans across two classes: **tumor** and **no tumor**."
      ],
      "metadata": {
        "id": "OasrOV7uejNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"Cayanaaa/BrainTumorDatasets\", name=\"binary\")"
      ],
      "metadata": {
        "id": "W0k7UgQXXNw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 View Class Label Mapping\n",
        "\n",
        "This command reveals the label names and their corresponding integer encodings used internally by the dataset.\n"
      ],
      "metadata": {
        "id": "qZLUXOc3wGQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds['train'].features['label'].names)"
      ],
      "metadata": {
        "id": "IwKwZYVTX57H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Extract Images and Labels from Dataset\n",
        "\n",
        "We extract the raw image and label pairs from the dataset for further processing.\n"
      ],
      "metadata": {
        "id": "T0X3CaW8X373"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Data\n",
        "train_data = ds['train']\n",
        "images = train_data['image']\n",
        "labels = train_data['label']"
      ],
      "metadata": {
        "id": "Z9H3Ma1vX3Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Stratified Train-Validation Split\n",
        "\n",
        "To ensure balanced class distribution across the training and validation sets, we perform a stratified split. This minimizes the risk of class imbalance during model training.\n"
      ],
      "metadata": {
        "id": "lDLInP2aYCL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_imgs, val_imgs, train_labels, val_labels = train_test_split(images, labels,\n",
        "                                                                  test_size=0.2,\n",
        "                                                                  stratify=labels,\n",
        "                                                                  random_state=42\n",
        "                                                                  )"
      ],
      "metadata": {
        "id": "Imp5BMfsYCoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Initialize Data Module\n",
        "\n",
        "We instantiate the **`BrainTumorDataModule`** with the prepared training and validation datasets.  \n",
        "This module handles data loading, preprocessing, and batching automatically during training and validation.\n",
        "\n",
        "**Parameters:**\n",
        "- **`train_data`** & **`val_data`** — Tuples containing image tensors and corresponding labels.\n",
        "- **`batch_size`** — Number of samples per batch during training/validation.\n",
        "- **`img_size`** — Target spatial size for resizing images before feeding them into the model.\n",
        "- **`num_workers`** — Number of subprocesses to use for data loading to speed up I/O operations.\n",
        "\n",
        "By using a **`LightningDataModule`**, we ensure a clean separation between the **data pipeline** and the **model logic**, improving code maintainability and reusability.\n"
      ],
      "metadata": {
        "id": "MkcHfO7GYGuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_module = BrainTumorDataModule(\n",
        "    train_data = (train_imgs, train_labels),\n",
        "    val_data = (val_imgs, val_labels),\n",
        "    batch_size = 64,\n",
        "    img_size = (224, 224),\n",
        "    num_workers = 4\n",
        ")"
      ],
      "metadata": {
        "id": "A8jYC6UAYHax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Warm-Up Training Phase\n"
      ],
      "metadata": {
        "id": "CDHAsBhBax7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Model Initialization for the Warm-Up Phase\n",
        "\n",
        "In this step, we initialize a **binary classifier** model using `DenseNet121` as the backbone.  \n",
        "During the **warm-up phase**, all pre-trained layers remain **frozen** to preserve the features learned from ImageNet.  \n",
        "Only the final **classification head** is trained, which helps stabilize the training process before fine-tuning deeper layers.\n",
        "\n",
        "**Configuration for this phase:**\n",
        "- **`learning_rate:`** `1e-3` — relatively high for faster convergence on the new classification head.\n",
        "- **`weight_decay:`** `1e-5` — small *L2 regularization* to prevent overfitting.\n",
        "- **`unfreeze_layers:`** `None` — ensures that only the classification head can be trained.\n",
        "\n",
        "> **Note:** The classifier will be unfrozen in later phases for fine-tuning deeper layers.\n"
      ],
      "metadata": {
        "id": "MnAU9ihHx-pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_warmup = DenseNetClassifierBinary(\n",
        "    learning_rate = 1e-3,\n",
        "    weight_decay = 1e-5,\n",
        "    unfreeze_layers = None\n",
        ")"
      ],
      "metadata": {
        "id": "Bvz6jX6rayQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Configuring Callbacks\n",
        "\n",
        "In this step, we set up the **callbacks** that will be used during model training.  \n",
        "Callbacks in PyTorch Lightning provide a mechanism to inject custom behavior at various stages of the training loop — such as saving checkpoints, early stopping, or scheduling learning rates.\n",
        "\n",
        "Here, we use the custom function `get_callbacks()` to create and configure the following:\n",
        "\n",
        "- **Model Checkpointing**  \n",
        "  Automatically saves the model's weights whenever the monitored metric (`val_loss`) improves.  \n",
        "  - **`dirpath`**: Path to store checkpoint files.  \n",
        "  - **`monitor`**: Metric used to decide if a new checkpoint should be saved (`val_loss` in this case).  \n",
        "  - **`mode`**: Set to `\"min\"` so that lower values of `val_loss` are considered better.  \n",
        "\n",
        "- **Early Stopping**  \n",
        "  Stops training early if the monitored metric does not improve after a defined patience period (`patience=3` here), preventing overfitting and saving time.\n",
        "\n",
        "> *By modularizing callbacks into a separate function (`get_callbacks()`), we maintain cleaner code and make it easier to reuse and adjust the configuration across multiple experiments.*\n"
      ],
      "metadata": {
        "id": "OTqoMZV1bI_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks_warmup = get_callbacks(\n",
        "    dirpath = CHECKPOINT_PATH,\n",
        "    monitor = 'val_loss',\n",
        "    mode = 'min',\n",
        "    patience = 3\n",
        ")"
      ],
      "metadata": {
        "id": "10jgvPqLbJlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Setup TensorBoard Logger for Warm-Up Phase\n",
        "\n",
        "In this step, we initialize the **TensorBoard logger** to track and visualize training metrics during the warm-up phase.\n",
        "\n",
        "- **`save_dir`** specifies the root directory where logs will be stored.  \n",
        "- Logs are saved inside a subfolder named `\"warmup\"` to keep warm-up training logs organized separately from other phases.  \n",
        "- This setup enables detailed monitoring of key metrics such as loss, accuracy, and learning rate using TensorBoard’s interactive web interface.\n",
        "\n",
        "> **Note:** The magic command `%load_ext tensorboard` is executed once to enable TensorBoard integration in this notebook session.  \n",
        "> After that, the `%tensorboard` command can be run multiple times to launch the TensorBoard UI pointing to the appropriate log directory without needing to reload the extension.\n"
      ],
      "metadata": {
        "id": "uzyxQ_dVbw0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger_warmup = TensorBoardLogger(\n",
        "    save_dir = os.path.join(SAVE_PATH, \"logs\"),\n",
        "    name = \"best_warmup_model\"\n",
        ")\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "pDqU0DJ0bxIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Configure Trainer for Warm-Up Phase\n",
        "\n",
        "This cell sets up the **PyTorch Lightning Trainer** which orchestrates the training loop.\n",
        "\n",
        "Key parameters:\n",
        "- **`max_epochs`**: The maximum number of training epochs.\n",
        "- **`accelerator`**: Automatically selects the best available device (GPU/CPU).\n",
        "- **`callbacks`**: Includes checkpointing and early stopping to optimize training.\n",
        "- **`logger`**: Enables logging of metrics to TensorBoard.\n",
        "- **`log_every_n_steps`**: Logs training metrics every 10 batches for timely monitoring.\n"
      ],
      "metadata": {
        "id": "NySnkjJ4b9-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_warmup = pl.Trainer(\n",
        "    max_epochs = 100,\n",
        "    accelerator = 'auto',\n",
        "    callbacks = callbacks_warmup,\n",
        "    logger = logger_warmup,\n",
        "    log_every_n_steps = 10\n",
        ")"
      ],
      "metadata": {
        "id": "uMp4QdS2b-R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Execute Warm-Up Training\n",
        "\n",
        "In this step, the training process for the warm-up phase is started using the configured Trainer.\n",
        "\n",
        "- The model (**`model_warmup`**) is trained with the prepared data module (**`data_module`**).\n",
        "- The training loop runs for up to **`max_epochs`** epochs or until early stopping criteria are met.\n",
        "- Training progress, metrics, and checkpoints are automatically handled by the Trainer and callbacks.\n",
        "- TensorBoard UI will be launched automatically, allowing you to monitor training metrics in real-time.\n"
      ],
      "metadata": {
        "id": "QZn-4cOB-DuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir {logger_warmup.log_dir}"
      ],
      "metadata": {
        "id": "UQ3UigD85XeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_warmup.fit(model_warmup, datamodule=data_module)"
      ],
      "metadata": {
        "id": "AzovCVJY-EIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 Finetune Training Phases"
      ],
      "metadata": {
        "id": "Fa7vjN7B-O8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Model Initialization for the Fine-Tuning Phase\n",
        "\n",
        "In this phase, we initialize the binary classifier model with selective layer unfreezing to allow fine-tuning.\n",
        "\n",
        "- **`learning_rate:`** Lowered to **`1e-5`** for more precise updates and to avoid disrupting previously learned features.\n",
        "- **`weight_decay:`** Reduced to **`1e-6`** for minimal regularization, allowing more flexibility during fine-tuning.\n",
        "- **`unfreeze_layers:`** Specific layers such as **`features.denseblock4`** and **`features.norm5`** are unfrozen to enable gradient updates, while other layers remain frozen.\n",
        "  \n",
        "> This strategy allows the model to adapt deeper feature representations to the new task while maintaining stability in earlier layers.\n"
      ],
      "metadata": {
        "id": "NzCYOGCM1urN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_finetune = DenseNetClassifierBinary(\n",
        "    learning_rate = 1e-5,\n",
        "    weight_decay = 1e-6,\n",
        "    unfreeze_layers = [\"features.denseblock4\", \"features.norm5\"]\n",
        ")"
      ],
      "metadata": {
        "id": "YcPwT9a--Qk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Configuring Callbacks for Fine-Tuning\n",
        "\n",
        "In this step, we set up the **callbacks** that will be used during the **fine-tuning** phase.  \n",
        "Callbacks in PyTorch Lightning provide a mechanism to inject custom behavior at various stages of the training loop — such as saving checkpoints, early stopping, or scheduling learning rates.\n",
        "\n",
        "Here, we use the custom function `get_callbacks()` to create and configure the following:\n",
        "\n",
        "- **Model Checkpointing**  \n",
        "  Automatically saves the model's weights whenever the monitored metric (`val_loss`) improves.  \n",
        "  - **`dirpath`**: Path to store checkpoint files (defined by `CHECKPOINT_PATH`).  \n",
        "  - **`monitor`**: Metric used to decide if a new checkpoint should be saved (`val_loss` in this case).  \n",
        "  - **`mode`**: Set to `\"min\"` so that lower values of `val_loss` are considered better.\n",
        "\n",
        "- **Early Stopping**  \n",
        "  Stops training early if the monitored metric does not improve after a defined patience period (`patience=3` here), preventing overfitting and saving time.\n",
        "\n",
        "> *By modularizing callbacks into a separate function (`get_callbacks()`), we maintain cleaner code and make it easier to reuse and adjust the configuration across multiple experiments.*\n"
      ],
      "metadata": {
        "id": "oo5P45e6_A2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks_finetune = get_callbacks(\n",
        "    dirpath = CHECKPOINT_PATH,\n",
        "    monitor = 'val_loss',\n",
        "    mode = 'min',\n",
        "    patience = 3\n",
        ")"
      ],
      "metadata": {
        "id": "5x-zs7eI_BK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Setup TensorBoard Logger for Fine-Tuning Phase\n",
        "\n",
        "This step initializes the **TensorBoard logger** to track and visualize training metrics during the fine-tuning phase.  \n",
        "\n",
        "- **`save_dir`** specifies the root directory where logs will be stored.  \n",
        "- Logs are saved inside a subfolder named `\"finetune\"` to keep fine-tuning training logs organized separately from other phases.\n",
        "\n",
        "Using TensorBoard enables easy monitoring of key metrics such as loss, accuracy, and learning rate through an interactive web interface.\n"
      ],
      "metadata": {
        "id": "ybAhvy3I_MRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger_finetune = TensorBoardLogger(\n",
        "    save_dir = os.path.join(SAVE_PATH, \"logs\"),\n",
        "    name = \"best_finetune_model\"\n",
        ")"
      ],
      "metadata": {
        "id": "93bCo6in_Mly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Configure Trainer for Warm-Up Phase\n",
        "\n",
        "This cell sets up the **PyTorch Lightning Trainer** which orchestrates the training loop.\n",
        "\n",
        "Key parameters:\n",
        "- **`max_epochs`**: The maximum number of training epochs.\n",
        "- **`accelerator`**: Automatically selects the best available device (GPU/CPU).\n",
        "- **`callbacks`**: Includes checkpointing and early stopping to optimize training.\n",
        "- **`logger`**: Enables logging of metrics to TensorBoard.\n",
        "- **`log_every_n_steps`**: Logs training metrics every 10 batches for timely monitoring.\n"
      ],
      "metadata": {
        "id": "Ck99CAOi_XSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_finetune = pl.Trainer(\n",
        "    max_epochs = 100,\n",
        "    accelerator = 'auto',\n",
        "    callbacks = callbacks_finetune,\n",
        "    logger = logger_finetune,\n",
        "    log_every_n_steps = 10\n",
        ")"
      ],
      "metadata": {
        "id": "ibnHwHT3_XjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5 Execute Fine-Tuning Training\n",
        "\n",
        "In this step, the training process for the fine-tuning phase is started using the configured Trainer.\n",
        "\n",
        "- The model (**`model_finetune`**) is trained with the prepared data module (**`data_module`**).\n",
        "- The training loop runs for up to **`max_epochs`** epochs or until early stopping criteria are met.\n",
        "- Training progress, metrics, and checkpoints are automatically handled by the Trainer and callbacks.\n",
        "- TensorBoard UI will be launched automatically, allowing you to monitor training metrics in real-time.\n"
      ],
      "metadata": {
        "id": "YRwwEGSq_jfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir {logger_finetune.log_dir}"
      ],
      "metadata": {
        "id": "ViztdSal564I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_finetune.fit(model_finetune, datamodule=data_module)"
      ],
      "metadata": {
        "id": "AglUKzLh_jwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Save and Export Fine-Tuned Model\n"
      ],
      "metadata": {
        "id": "qBNoSFmZ_rMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Load Best Checkpoint After Fine-Tuning\n",
        "\n",
        "After completing the fine-tuning process, we retrieve the path of the **best checkpoint** automatically saved by the `ModelCheckpoint` callback.\n",
        "\n",
        "- `callbacks_finetune[0].best_model_path` accesses the best checkpoint based on the monitored metric (`val_loss` in this case).\n",
        "- Using PyTorch Lightning's `load_from_checkpoint` method, the model is reloaded with weights from this best checkpoint.\n",
        "- This loaded model can then be saved as a `.pth` file for easy storage and future use.\n",
        "- The saved `.pth` model file can be used later for **evaluation** and **inference** without needing to retrain or reload the entire checkpoint.\n",
        "\n",
        "This workflow ensures a clean separation between training, model saving, and later deployment or analysis.\n"
      ],
      "metadata": {
        "id": "5pSqpZYS8vZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_checkpoint_path = callbacks_finetune[0].best_model_path"
      ],
      "metadata": {
        "id": "bOw0shHxCeCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = DenseNetClassifierBinary.load_from_checkpoint(best_checkpoint_path)"
      ],
      "metadata": {
        "id": "AZ8MDgV2CoR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Save Fine-Tuned Model and Mount Google Drive\n",
        "\n",
        "In this step, we perform two important actions:\n",
        "\n",
        "- **Mount Google Drive**  \n",
        "  Using `drive.mount('/content/drive')` to connect the Colab environment with your Google Drive, enabling you to save and access files persistently.\n",
        "\n",
        "- **Save Model Weights**  \n",
        "  The fine-tuned model's parameters (weights) are saved as a `.pth` file using `torch.save()`.  \n",
        "  - `best_model.state_dict()` extracts the model's state dictionary containing all learnable parameters.  \n",
        "  - The file is saved at the specified `PROJECT_PATH` with the name `best_ft_braTS_binary.pth`.  \n",
        "  - Saving the model weights separately allows lightweight storage and easy loading for future inference or evaluation without the full training checkpoint overhead.\n"
      ],
      "metadata": {
        "id": "274OQWL2CnnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZcKwK0LYYjmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(best_model.state_dict(), f\"{PROJECT_PATH}/best_ft_braTS_binary.pth\")"
      ],
      "metadata": {
        "id": "h8pW2chtHrJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "> This notebook marks an important milestone in my personal learning journey as I transition from vanilla PyTorch to PyTorch Lightning.  \n",
        "\n",
        "Through this experience, I have gained valuable insights into:  \n",
        "- How to organize deep learning projects modularly for improved clarity and maintainability.  \n",
        "- The practical benefits of PyTorch Lightning in simplifying training workflows, including built-in support for checkpointing, logging, and callbacks.  \n",
        "- Implementing a two-phase training strategy (warm-up and fine-tuning) to effectively adapt pre-trained models to new tasks.  \n",
        "- Using TensorBoard for real-time monitoring and Google Drive for seamless model persistence in cloud environments.\n",
        "\n",
        "> While focused on training and model saving, this notebook lays a solid foundation for future evaluation and inference stages, which will be handled separately to keep workflows clean and manageable.\n",
        "\n",
        "This hands-on exploration not only deepens my understanding of deep learning engineering best practices but also builds a professional and reproducible pipeline that can be extended or adapted for other projects and users.\n"
      ],
      "metadata": {
        "id": "CCOOyhFI-MLY"
      }
    }
  ]
}