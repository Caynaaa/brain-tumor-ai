{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ9bGeur_DmU"
      },
      "source": [
        "# Multiclass Brain Tumor Classifier — Runner Notebook\n",
        "\n",
        "> **Purpose:** This notebook serves as the execution script for the **Multiclass brain tumor classification model**.  \n",
        "> All core components (model architecture, data processing, training pipeline) are defined in separate Python modules to maintain a clean and modular project structure.\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook is part of the **Brain Tumor AI** project, focusing on **multiclass classification** of medical images **(notumor, pituitary, meningioma, glioma)**.  \n",
        "It is designed to:\n",
        "- Load and configure the modular components (model, data module, transforms, helpers, callbacks, loggers).\n",
        "- Execute the training process using **PyTorch Lightning**.\n",
        "- Save the trained model for inference.\n",
        "\n",
        "By separating logic into `.py` files, the project ensures:\n",
        "- **Reusability:** Components can be reused across multiple experiments.\n",
        "- **Maintainability:** Easier debugging and updates.\n",
        "- **Clarity:** The notebook focuses on workflow and results, not implementation details.\n",
        "\n",
        "\n",
        "> **Note:** This project is for learning and portfolio purposes only — not for clinical use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies & Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Install Dependencies\n",
        "Install the required packages to ensure the notebook runs without missing dependencies.\n",
        "\n",
        "- **`datasets`** — Dataset handling and loading utilities.  \n",
        "- **`fsspec`** — File system interface for remote/local storage.  \n",
        "- **`pytorch-lightning`** — High-level PyTorch framework for training.  \n",
        "- **`albumentations`** — Advanced image augmentation library.  \n",
        "- **`torchmetrics`** — Standardized metrics for PyTorch.\n",
        "\n",
        "> Skip this step if the environment already has these packages installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs3-olNV-ajm"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U datasets fsspec pytorch-lightning albumentations torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJIS-cI9_D60"
      },
      "source": [
        "### 1.2 Import Required Libraries\n",
        "\n",
        "Below are the required libraries and modules used in this notebook:\n",
        "\n",
        "- **os, sys** — For file and system path handling.\n",
        "- **torch** — PyTorch core library for deep learning operations.\n",
        "- **pytorch_lightning** — High-level wrapper for PyTorch to simplify training loops.\n",
        "- **scikit-learn (train_test_split, compute_class_weight)** — For dataset splitting and class weight computation.\n",
        "- **google.colab.drive** — To mount Google Drive and access stored datasets/models.\n",
        "- **datasets.load_dataset** — To load datasets in various formats from the Hugging Face Datasets library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGwjndZ1_EOk"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLwGKEX__LMB"
      },
      "source": [
        "- **Mount Google Drive**  \n",
        "  Using `drive.mount('/content/drive')` to connect the Colab environment with your Google Drive, enabling you to save and access files persistently.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZtRtFuV_Lh5"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mojph5AO_O9W"
      },
      "source": [
        "## 2. Define Project paths and Import Custom Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Configure Directory Paths  \n",
        "Here, we define the key directory paths used throughout the project:  \n",
        "\n",
        "- **`CHECKPOINT_PATH`** — Location where model checkpoints will be saved and loaded from.  \n",
        "- **`PROJECT_PATH`** — Root path of the project, used as a base reference for file operations.  \n",
        "- **`SAVE_PATH`** — Directory for storing final outputs, such as trained models.  \n",
        "\n",
        "The `PROJECT_PATH` is appended to `sys.path` to make sure Python can locate and import the project modules without issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQfCI_s0_POy"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/MyProject/brain-tumor-ai/Models/2D_Classifier_Multiclass/checkpoint\"\n",
        "\n",
        "PROJECT_PATH = \"/content/drive/MyDrive/MyProject/brain-tumor-ai/Models/2D_Classifier_Multiclass\"\n",
        "\n",
        "SAVE_PATH = \"/content/drive/MyDrive/MyProject/brain-tumor-ai/Models/2D_Classifier_Multiclass/save_models\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Add Project Directory to Python Path**\n",
        "\n",
        "    To ensure that custom modules can be imported without issues, the project directory (`PROJECT_PATH`) is appended to the Python system path (`sys.path`).  \n",
        "This step allows Python to locate and load modules defined in your project folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEXtAzw2_1Nl"
      },
      "outputs": [],
      "source": [
        "if PROJECT_PATH not in sys.path:\n",
        "  sys.path.append(PROJECT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0scah2PA_95Y"
      },
      "source": [
        "### 2.2 Import Custom Modules\n",
        "\n",
        "This section imports the custom Python modules that define the model architecture, data pipeline, training callbacks, and helper functions.  \n",
        "By keeping these components in separate files, the project maintains a clean and modular structure.\n",
        "\n",
        "- **DenseNetClassifierMulticlass** — Custom PyTorch Lightning model for multiclass brain tumor classification.  \n",
        "- **BrainTumorDataModule** — Handles data loading, preprocessing, and batching using PyTorch Lightning's DataModule structure.  \n",
        "- **get_callbacks** — Retrieves predefined training callbacks such as model checkpointing and early stopping.  \n",
        "- **get_logger** — Logger utility for experiment tracking and visualization.  \n",
        "- **utils** — Utility functions to ensure reproducibility and dataset conversion.\n",
        "\n",
        "> Keeping core logic in separate `.py` files improves reusability, maintainability, and"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "160461o6_-M6"
      },
      "outputs": [],
      "source": [
        "from module import DenseNetClassifierMulticlass\n",
        "from datamodule import BrainTumorDataModule\n",
        "from callbacks import get_callbacks\n",
        "from logger import get_logger\n",
        "from utils import set_seed, hf_dataset_to_tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmaCz3tTADad"
      },
      "source": [
        "## 3. Define Seed, Load and Prepare Raw Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Set Random Seed\n",
        "\n",
        "To ensure reproducibility of results, a fixed random seed is set at the beginning of the data preparation process.  \n",
        "By setting the seed, all operations involving randomness (such as data shuffling, train-test splitting, and weight initialization) will produce the same outcome each time the notebook is executed. This step is crucial for debugging and for achieving consistent experimental results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuI9rKqCBWE_"
      },
      "outputs": [],
      "source": [
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS5x-DQLBWYF"
      },
      "source": [
        "### 3.2 Load Dataset from Hugging Face\n",
        "\n",
        "We load the dataset directly using the `datasets` library. The dataset contains labeled 2D brain MRI scans across **four classes:** **notumor**, **pituitary**, **meningioma**, **glioma**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N_3DW1GBptd"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"Cayanaaa/BrainTumorDatasets\", name=\"multiclass\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bijPo7WFB60A"
      },
      "source": [
        "### 3.3 View Class Label Mapping\n",
        "\n",
        "This command reveals the label names and their corresponding integer encodings used internally by the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuZRV3fuB7Js"
      },
      "outputs": [],
      "source": [
        "print(ds['train'].features['label'].names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nhvv7DW6C4mv"
      },
      "source": [
        "### 3.4 Convert Hugging Face Dataset to Tuples\n",
        "\n",
        "The raw Hugging Face dataset is converted into Python tuples containing image data and corresponding labels.  \n",
        "This step simplifies further processing and integration with PyTorch and custom data modules.\n",
        "\n",
        "- **`hf_dataset_to_tuple`** — Utility function that extracts images and labels from the dataset, returning them as separate lists or arrays for easy manipulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aysi9Pw8C44U"
      },
      "outputs": [],
      "source": [
        "train_data = ds['train']\n",
        "\n",
        "images, labels = hf_dataset_to_tuple(train_data, image_key='image', label_key='label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL5RXo5pDHRZ"
      },
      "source": [
        "### 3.5 Stratified Train-Validation Split\n",
        "\n",
        "To ensure a balanced class distribution between the training and validation sets, we perform a **stratified split** using `train_test_split` from scikit-learn.  \n",
        "This approach helps prevent class imbalance issues during model training and evaluation.\n",
        "\n",
        "- **`train_imgs`, `val_imgs`** — Image data for training and validation.\n",
        "- **`train_labels`, `val_labels`** — Corresponding labels for each split.\n",
        "\n",
        "The split uses a fixed `random_state` for reproducibility and the `stratify` parameter to maintain class proportions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR-zOOQWDHiU"
      },
      "outputs": [],
      "source": [
        "train_imgs, val_imgs, train_labels, val_labels = train_test_split(\n",
        "    images, labels,\n",
        "    test_size = 0.2,\n",
        "    random_state = 42,\n",
        "    stratify = labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.6 Compute Class Weights\n",
        "\n",
        "To address potential class imbalance in the dataset, we calculate class weights using `compute_class_weight` from scikit-learn.  \n",
        "These weights are used during model training to ensure that each class contributes equally to the loss function, improving overall model performance and fairness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = np.array(labels)\n",
        "\n",
        "class_weight = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(labels),\n",
        "    y=labels\n",
        ")\n",
        "\n",
        "class_weight = torch.tensor(class_weight, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tR6blK0Db5f"
      },
      "source": [
        "## 4. Data Module "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Initialize Data Module\n",
        "\n",
        "The `BrainTumorDataModule` is instantiated to handle data loading, preprocessing, and batching for both training and validation sets.  \n",
        "This module streamlines the data pipeline, ensuring efficient and reproducible data handling throughout the training process.\n",
        "\n",
        "- **`train_data`** — Tuple containing training images and labels.\n",
        "- **`val_data`** — Tuple containing validation images and labels.\n",
        "- **`batch_size`** — Number of samples per batch during training.\n",
        "- **`num_worker`** — Number of subprocesses used for data loading.\n",
        "\n",
        "Using a custom DataModule improves modularity and simplifies integration with PyTorch Lightning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gqH_9XaDcUc"
      },
      "outputs": [],
      "source": [
        "data_module = BrainTumorDataModule(\n",
        "    train_data = (train_imgs, train_labels),\n",
        "    val_data = (val_imgs, val_labels),\n",
        "    batch_size = 64,\n",
        "    num_worker = 4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8aCoUKXD67s"
      },
      "source": [
        "## 5. Warm-up Training Phases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Model Initialization for the Warm-Up Phase\n",
        "\n",
        "In this step, we initialize a multi-class brain tumor classifier using the DenseNet121 backbone.\n",
        "For the warm-up phase, all pre-trained layers remain frozen—only the final classifier head can be trained.\n",
        "This strategy allows the model to adapt its output layers to our specific tumor class while preserving robust features learned from large-scale datasets (e.g., ImageNet).\n",
        "\n",
        "Why freeze the backbone?\n",
        "\n",
        "Freezing the backbone during warm-up:\n",
        "- Prevents the perturbation of valuable pre-trained features.\n",
        "- Allows the classifier head to specialize for the new task.\n",
        "- Stabilizes training before deeper fine-tuning.\n",
        "\n",
        "Configuration:\n",
        "- `learning_rate`: Controls the learning rate for the classification head.\n",
        "- `weight_decay`: Regularization to prevent overfitting.\n",
        "- `unfreeze_layers`: Set to `None` to keep all backbone layers frozen **(except the classifier).**\n",
        "- `class_weight`: Handles class imbalance during training.\n",
        "\n",
        "This initialization establishes the foundation for effective transfer learning and prepares the model for further fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhLCmZ52D7NY"
      },
      "outputs": [],
      "source": [
        "model_warmup = DenseNetClassifierMulticlass(\n",
        "    learning_rate = 1e-3,\n",
        "    weight_decay = 1e-5,\n",
        "    unfreeze_layers = None,\n",
        "    class_weight = class_weight\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9VZRA_XEP0O"
      },
      "source": [
        "### 5.2 Configuring Callbacks\n",
        "\n",
        "In this step, we set up the **callbacks** that will be used during model training.  \n",
        "Callbacks in PyTorch Lightning provide a mechanism to inject custom behavior at various stages of the training loop — such as saving checkpoints, early stopping, or scheduling learning rates.\n",
        "\n",
        "Here, we use the custom function `get_callbacks()` to create and configure the following:\n",
        "\n",
        "- **Model Checkpointing**  \n",
        "  Automatically saves the model's weights whenever the monitored metric (`val_loss`) improves.  \n",
        "  - **`dirpath`**: Path to store checkpoint files.  \n",
        "  - **`monitor`**: Metric used to decide if a new checkpoint should be saved (`val_loss` in this case).  \n",
        "  - **`mode`**: Set to `\"min\"` so that lower values of `val_loss` are considered better.  \n",
        "\n",
        "- **Early Stopping**  \n",
        "  Stops training early if the monitored metric does not improve after a defined patience period (`patience=3` here), preventing overfitting and saving time.\n",
        "\n",
        "> *By modularizing callbacks into a separate function (`get_callbacks()`), we maintain cleaner code and make it easier to reuse and adjust the configuration across multiple experiments.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K5woY5aEQD4"
      },
      "outputs": [],
      "source": [
        "callbacks_warmup = get_callbacks(\n",
        "    dirpath = CHECKPOINT_PATH,\n",
        "    monitor = 'val_loss',\n",
        "    mode = 'min',\n",
        "    patience = 3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnSQfAd8Ep2F"
      },
      "source": [
        "### 5.3 Configure Logger for Warm-Up Phase\n",
        "\n",
        "To track training progress and save experiment logs, we initialize a logger using the custom `get_logger()` function.  \n",
        "This logger records metrics, checkpoints, and other relevant information for the warm-up phase.\n",
        "\n",
        "- **`log_dir`** — Directory where logs will be stored.\n",
        "- **`name`** — Unique identifier for the logger, useful for organizing multiple experiments.\n",
        "\n",
        "Consistent logging ensures reproducibility and simplifies analysis of model performance across different training runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euwT4jCKEqIb"
      },
      "outputs": [],
      "source": [
        "logger_warmup = get_logger(\n",
        "    log_dir = PROJECT_PATH/logs,\n",
        "    name = \"best_warmup_model_checkpoint\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaicLTkNFWyi"
      },
      "source": [
        "### 5.4 Configure Trainer for Warm-Up Phase\n",
        "\n",
        "This cell sets up the **PyTorch Lightning Trainer** which orchestrates the training loop.\n",
        "\n",
        "Configuration:\n",
        "- **`max_epochs`**: The maximum number of training epochs.\n",
        "- **`accelerator`**: Automatically selects the best available device (GPU/CPU).\n",
        "- **`callbacks`**: Includes checkpointing and early stopping to optimize training.\n",
        "- **`logger`**: Enables logging of metrics to TensorBoard.\n",
        "- **`log_every_n_steps`**: Logs training metrics every 10 batches for timely monitoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqQNPtn4FXEb"
      },
      "outputs": [],
      "source": [
        "trainer_warmup = pl.Trainer(\n",
        "    max_epochs = 200,\n",
        "    accelerator = 'gpu',\n",
        "    precision = '16-mixed',\n",
        "    callbacks = callbacks_warmup,\n",
        "    logger = logger_warmup,\n",
        "    log_every_n_step = 10,\n",
        "    device = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 Execute Warm-Up Training\n",
        "\n",
        "In this step, the training process for the warm-up phase is started using the configured Trainer.\n",
        "\n",
        "- The model (**`model_warmup`**) is trained with the prepared data module (**`data_module`**).\n",
        "- The training loop runs for up to **`max_epochs`** epochs or until early stopping criteria are met.\n",
        "- Training progress, metrics, and checkpoints are automatically handled by the Trainer and callbacks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-3A1HnwF0K5"
      },
      "outputs": [],
      "source": [
        "trainer_warmup.fit(model_warmup, datamodule = data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkli4_k2F9Sz"
      },
      "source": [
        "## 6. Finetune Training Phases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Model Initialization for the Fine-Tuning Phase\n",
        "\n",
        "In this phase, the multi-class brain tumor classifier model is initialized for fine-tuning.\n",
        "\n",
        "Unlike the warm-up phase, some backbone layers (e.g., `features.denseblock4`, `features.norm5`) will be unfrozen to allow their weights to be updated during training.\n",
        "\n",
        "**Goals of fine-tuning:**\n",
        "- Improve feature representation to be more specific to the brain tumor dataset.\n",
        "- Improve model accuracy and generalization.\n",
        "\n",
        "**Configuration:**\n",
        "- `learning_rate`: Smaller to avoid drastic changes to pre-trained weights.\n",
        "- `weight_decay`: Regularization to prevent overfitting.\n",
        "- `unfreeze_layers`: List of backbone layers to be unfrozen.\n",
        "- `class_weight`: Class imbalance handling.\n",
        "\n",
        "This strategy allows the model to leverage pre-trained knowledge while optimally tailoring features to the brain tumor classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxJYokhhF9jF"
      },
      "outputs": [],
      "source": [
        "model_finetune = DenseNetClassifierMulticlass(\n",
        "    learning_rate = 1e-5,\n",
        "    weight_decay = 1e-6,\n",
        "    unfreeze_layers = [\"features.denseblock4\", \"features.norm5\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2SQK3V2GRqN"
      },
      "source": [
        "### 6.2 Configuring Callbacks for Fine-Tuning Phase\n",
        "\n",
        "In this step, we set up the **callbacks** for the fine-tuning phase of model training.  \n",
        "Callbacks in PyTorch Lightning allow us to automate important tasks such as saving model checkpoints and stopping training early when improvements plateau.\n",
        "\n",
        "We use the custom `get_callbacks()` function to configure:\n",
        "\n",
        "- **Model Checkpointing**  \n",
        "    Automatically saves the model's weights whenever the monitored metric (`val_loss`) improves during fine-tuning.  \n",
        "    - **`dirpath`**: Directory to store checkpoint files.  \n",
        "    - **`monitor`**: Metric used to determine if a new checkpoint should be saved (`val_loss`).  \n",
        "    - **`mode`**: `\"min\"` so that lower values of `val_loss` are considered better.  \n",
        "\n",
        "- **Early Stopping**  \n",
        "    Stops training if `val_loss` does not improve after a set patience period (`patience=3`), helping to prevent overfitting and save resources.\n",
        "\n",
        "> *By modularizing callbacks into a separate function (`get_callbacks()`), we keep the code clean and make it easy to reuse or adjust callback settings for different training phases.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wskZwRsGSBh"
      },
      "outputs": [],
      "source": [
        "callbacks_finetune = get_callbacks(\n",
        "    dirpath = CHECKPOINT_PATH,\n",
        "    monitor = 'val_loss',\n",
        "    mode = 'min',\n",
        "    patience = 3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK1oS63XGaEv"
      },
      "source": [
        "### 6.3 Configure Logger for Fine-Tuning Phase\n",
        "\n",
        "To monitor and record the training progress during the fine-tuning phase, we initialize a logger using the custom `get_logger()` function.  \n",
        "This logger will save experiment logs, metrics, and checkpoints specific to the fine-tuning process.\n",
        "\n",
        "- **`log_dir`** — Directory where logs for the fine-tuning phase will be stored.\n",
        "- **`name`** — Unique identifier for the logger, helping organize and distinguish between different training runs.\n",
        "\n",
        "Consistent logging during fine-tuning ensures reproducibility and simplifies the analysis and comparison of model performance across different experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T92aZn0BGabX"
      },
      "outputs": [],
      "source": [
        "logger_finetune = get_logger(\n",
        "    log_dir = PROJECT_PATH/logs,\n",
        "    name = \"best_finetune_model_checkpoint\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 Configure the Trainer for Fine-Tuning\n",
        "\n",
        "In this step, we configure the PyTorch Lightning Trainer for fine-tuning.\n",
        "\n",
        "Configuration:\n",
        "- **`max_epochs`**: Maximum number of training epochs.\n",
        "- **`accelerator`**: Selects the best available device (GPU/CPU).\n",
        "- **`precision`**: Mixed precision for training efficiency.\n",
        "- **`callbacks`**: Includes checkpointing and early stopping to optimize training.\n",
        "- **`logger`**: Logs metrics to TensorBoard.\n",
        "- **`log_every_n_step`**: Logs every 10 batches for better monitoring.\n",
        "\n",
        "This trainer will run the fine-tuning process on the model with configurations adjusted to improve performance on the multi-class brain tumor dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvHQbiSwGfjV"
      },
      "outputs": [],
      "source": [
        "trainer_finetune = pl.Trainer(\n",
        "    max_epochs = 200,\n",
        "    accelerator = 'gpu',\n",
        "    precision = '16-mixed',\n",
        "    callbacks = callbacks_finetune,\n",
        "    logger = logger_finetune,\n",
        "    log_every_n_step = 10,\n",
        "    device = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0q50E4Gm4L"
      },
      "source": [
        "### 6.5 Execute Fine-Tuning Training\n",
        "\n",
        "In this step, the training process for the fine-tuning phase is run using the configured Trainer.\n",
        "\n",
        "- The model (**`model_finetune`**) will be trained using the prepared data module (**`data_module`**).\n",
        "- The training process runs until **`max_epochs`** is reached or until the early stopping criterion is met.\n",
        "- Training progress, metrics, and checkpoints will be automatically handled by the Trainer and callbacks.\n",
        "\n",
        "The purpose of this fine-tuning phase is to optimize the model's feature representation to be more specific to the multiclass brain tumor dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTVKUazgGnO6"
      },
      "outputs": [],
      "source": [
        "trainer_finetune(model_finetune, datamodule=data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ41Gn-iGtiM"
      },
      "source": [
        "## 7. Load and Save checkpoint and best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Load Best Model Checkpoint\n",
        "\n",
        "After training, we load the path to the best model checkpoint saved during fine-tuning.  \n",
        "This checkpoint contains the weights of the model that achieved the lowest validation loss.\n",
        "\n",
        "- **Print Best Checkpoint Path:**  \n",
        "    Display the file path of the best checkpoint for reference and verification.\n",
        "\n",
        "- **Assign Best Checkpoint Path:**  \n",
        "    Store the best checkpoint path in a variable for subsequent loading and inference.\n",
        "\n",
        "This step ensures that we use the most optimal model for evaluation and deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT2t9gqGIHK8"
      },
      "outputs": [],
      "source": [
        "print(callbacks_finetune[0].best_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U8V1KTTGt0T"
      },
      "outputs": [],
      "source": [
        "best_checkpoint_model_path = callbacks_finetune[0].best_model_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozG__aIIG6Rw"
      },
      "source": [
        "### 7.2 Load Best Model and Save Final Weights\n",
        "\n",
        "After identifying the best checkpoint from the fine-tuning phase, we proceed with the following steps:\n",
        "\n",
        "- **Load Best Model from Checkpoint:**  \n",
        "    The model is reconstructed using the weights stored in the best checkpoint file (`best_checkpoint_model_path`).  \n",
        "    This ensures that all parameters reflect the optimal state achieved during training.\n",
        "\n",
        "- **Save Final Model Weights:**  \n",
        "    The loaded model's weights are saved as a `.pth` file in the project directory.  \n",
        "    This file can be used for future inference, deployment, or further experimentation.\n",
        "\n",
        "By saving the final model weights, we ensure reproducibility and simplify downstream usage for clinical or research applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPyQqlkQG6g1"
      },
      "outputs": [],
      "source": [
        "best_model = DenseNetClassifierMulticlass.load_from_checkpoint(best_checkpoint_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lu28PyHHFt9"
      },
      "outputs": [],
      "source": [
        "torch.save(best_model.state_dict(), f\"{PROJECT_PATH}/best_ft_braTS_multiclass.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9dkYURhHGsK"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "> This notebook marks a significant milestone in my personal learning journey as I transitioned from standard PyTorch to PyTorch Lightning.\n",
        "\n",
        "Through this experience, I gained valuable insights into:\n",
        "- How to manage deep learning projects modularly for increased clarity and maintainability.\n",
        "- The practical benefits of PyTorch Lightning in simplifying training workflows, including built-in support for checkpointing, logging, and callbacks.\n",
        "- Implementing a two-phase training strategy (warm-up and fine-tune) to effectively adapt pre-trained models to new tasks.\n",
        "- Recognizing and correcting previous errors in the binary runner, resulting in cleaner logic and more reliable metric tracking.\n",
        "\n",
        "> While focused on model training and storage, this notebook lays a solid foundation for the future evaluation and inference phases, which will be handled separately to keep the workflow clean and manageable.\n",
        "\n",
        "This hands-on exploration not only deepened my understanding of deep learning engineering best practices but also established a professional and reproducible body of work that can be extended or adapted for other projects and users."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
